{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24021097",
      "metadata": {
        "id": "24021097"
      },
      "source": [
        "# Taller I2C: Entrenamiento con LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021210c5",
      "metadata": {
        "id": "021210c5"
      },
      "source": [
        "## Instalación de dependencias y preparación del kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e23f3c",
      "metadata": {
        "id": "c5e23f3c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U torch=='2.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6bfb66",
      "metadata": {
        "id": "da6bfb66",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U accelerate=='0.25.0' peft=='0.7.1' bitsandbytes=='0.41.3.post2' transformers=='4.36.1' trl=='0.7.4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BFJ5gM8MX447",
      "metadata": {
        "id": "BFJ5gM8MX447"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install pytorch_lightning\n",
        "!pip install datasets\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a089ac66",
      "metadata": {
        "id": "a089ac66"
      },
      "source": [
        "#### Limpiar caché"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67fcc94",
      "metadata": {
        "id": "c67fcc94"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41d4c66",
      "metadata": {
        "id": "d41d4c66"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2745bb52",
      "metadata": {
        "id": "2745bb52"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pytorch_lightning import seed_everything\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging,\n",
        "                          EarlyStoppingCallback)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9074a714",
      "metadata": {
        "id": "9074a714"
      },
      "outputs": [],
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "seed_everything(42, workers=True)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbc6a73",
      "metadata": {
        "id": "cdbc6a73"
      },
      "outputs": [],
      "source": [
        "# Comprobar GPU\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(f'GPU detected. Currently using: \"{torch.cuda.get_device_name(0)}\"')\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('Currently using CPU. To utilize GPU acceleration, change the runtime type in the \\'runtime\\' tab.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c2500f",
      "metadata": {
        "id": "c4c2500f"
      },
      "source": [
        "## Preparación del conjunto de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce26f8b",
      "metadata": {
        "id": "3ce26f8b"
      },
      "source": [
        "#### Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9oiDtY0vZwo0",
      "metadata": {
        "id": "9oiDtY0vZwo0"
      },
      "outputs": [],
      "source": [
        "# Montar directorio de drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cw3JdTXu39wd",
      "metadata": {
        "id": "Cw3JdTXu39wd"
      },
      "outputs": [],
      "source": [
        "# # Dataset Refugiados\n",
        "# train_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/refugiados_train_df.csv\"\n",
        "# test_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/refugiados_test_df.csv\"\n",
        "# valid_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/refugiados_valid_df.csv\"\n",
        "# campo_texto = 'text'\n",
        "# campo_etiqueta = 'label'\n",
        "# clase_0 = 'NO'\n",
        "# clase_1 = 'SI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RASe3egw4DmH",
      "metadata": {
        "id": "RASe3egw4DmH"
      },
      "outputs": [],
      "source": [
        "# Dataset Alimenticio\n",
        "train_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/train_df.csv\"\n",
        "test_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/test_df.csv\"\n",
        "valid_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/valid_df.csv\"\n",
        "campo_texto = 'text'\n",
        "campo_etiqueta = 'label'\n",
        "clase_0 = '0'\n",
        "clase_1 = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UteYxcry4Knj",
      "metadata": {
        "id": "UteYxcry4Knj"
      },
      "outputs": [],
      "source": [
        "# # Dataset HomoMEX\n",
        "# train_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/homomex_train_df.csv\"\n",
        "# test_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/homomex_test_df.csv\"\n",
        "# valid_filename = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/data/homomex_valid_df.csv\"\n",
        "# campo_texto = 'content'\n",
        "# campo_etiqueta = 'label'\n",
        "# clase_0 = 'NP'\n",
        "# clase_1 = 'P'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef75c86",
      "metadata": {
        "id": "bef75c86"
      },
      "outputs": [],
      "source": [
        "# Cargar los conjuntos de datos de entrenamiento, prueba y validación\n",
        "def load_data(train_file, test_file, valid_file):\n",
        "    X_train = pd.read_csv(train_file, encoding = \"utf-8\", encoding_errors = \"replace\", sep = ',')[[campo_texto, campo_etiqueta]]\n",
        "    X_test = pd.read_csv(test_file, encoding = \"utf-8\", encoding_errors = \"replace\", sep = ',')[[campo_texto, campo_etiqueta]]\n",
        "    X_eval = pd.read_csv(valid_file, encoding = \"utf-8\", encoding_errors = \"replace\", sep = ',')[[campo_texto, campo_etiqueta]]\n",
        "    return X_train, X_test, X_eval\n",
        "\n",
        "train_df, test_df, valid_df = load_data(train_filename, test_filename, valid_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0zzk-B9VWc0R",
      "metadata": {
        "id": "0zzk-B9VWc0R"
      },
      "outputs": [],
      "source": [
        "# def filter_classes(df, classes_to_keep):\n",
        "#     return df[df[campo_etiqueta].isin(classes_to_keep)]\n",
        "\n",
        "# # Filtrar datasets para quedarnos con las clases 0 y 1\n",
        "# train_df = filter_classes(train_df, [clase 0, clase 1])\n",
        "# test_df = filter_classes(test_df, [clase 0, clase 1])\n",
        "# valid_df = filter_classes(valid_df, [clase 0, clase 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QhJ_YRsPrDny",
      "metadata": {
        "id": "QhJ_YRsPrDny"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, X_eval = train_df, test_df, valid_df\n",
        "Y_true = X_test[campo_etiqueta]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1242c0ad",
      "metadata": {
        "id": "1242c0ad"
      },
      "source": [
        "#### Mezclar y reorganizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5ffa8b",
      "metadata": {
        "id": "fd5ffa8b"
      },
      "outputs": [],
      "source": [
        "# Mezclar y reiniciar índices de los conjuntos de datos\n",
        "def shuffle_and_reset_index(data, seed = 10):\n",
        "    return data.sample(frac = 1, random_state = seed).reset_index(drop = True)\n",
        "\n",
        "X_train = shuffle_and_reset_index(X_train)\n",
        "X_eval = shuffle_and_reset_index(X_eval)\n",
        "X_test = X_test.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c874e7b",
      "metadata": {
        "id": "3c874e7b"
      },
      "source": [
        "#### Visualización de la distribución de clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a63bbdd",
      "metadata": {
        "id": "0a63bbdd"
      },
      "outputs": [],
      "source": [
        "# Mostrar la distribución de clases en los conjuntos de datos\n",
        "def show_class_distribution(data, name):\n",
        "    print(f\"\\nDistribución de clases en el conjunto de {name}:\")\n",
        "    print(data[campo_etiqueta].value_counts())\n",
        "\n",
        "show_class_distribution(X_train, \"entrenamiento\")\n",
        "show_class_distribution(X_test, \"prueba\")\n",
        "show_class_distribution(X_eval, \"validación\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d2486e",
      "metadata": {
        "id": "81d2486e"
      },
      "source": [
        "#### Generación de Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65abe835",
      "metadata": {
        "id": "65abe835"
      },
      "outputs": [],
      "source": [
        "i_prompt = 0\n",
        "prompts = [ f\"\"\"\n",
        "            [INST]\n",
        "            Analiza el siguiente tweet para determinar si la persona que lo escribió muestra indicios de un trastorno alimenticio.\n",
        "            Considera el contenido, tono y posibles referencias a hábitos alimenticios, percepción corporal o conducta relacionada con la alimentación.\n",
        "            Devuelve exclusivamente **solo** la etiqueta \"1\" si hay signos de un trastorno alimenticio, o \"0\" si no los hay. No incluyas texto adicional.\n",
        "            [/INST]\"\"\",\n",
        "            f\"\"\"\n",
        "            [INST]\n",
        "            Analiza el siguiente tweet para determinar si contiene odio o no.\n",
        "            Devuelve exclusivamente **solo** la etiqueta \"SI\" si el tweet incluye odio o \"NO\" si no lo incluye, sin texto adicional.\n",
        "            [/INST]\n",
        "            \"\"\",\n",
        "          ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc68516",
      "metadata": {
        "id": "2bc68516"
      },
      "outputs": [],
      "source": [
        "# Funciones para generar los prompts de entrenamiento y prueba\n",
        "def generate_prompt(data_point, prompt):\n",
        "    return f\"\"\"\n",
        "            {prompt}\n",
        "\n",
        "            [TWEET: {data_point[campo_texto]}] = {data_point[campo_etiqueta]} \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point, prompt):\n",
        "    return f\"\"\"\n",
        "            {prompt}\n",
        "\n",
        "            [{data_point[campo_texto]}] = \"\"\".strip()\n",
        "\n",
        "# Aplicar la generación de prompts a los conjuntos de datos\n",
        "X_train = pd.DataFrame(X_train.apply(lambda row: generate_prompt(row, prompts[i_prompt]), axis=1), columns=[campo_texto])\n",
        "X_eval = pd.DataFrame(X_eval.apply(lambda row: generate_prompt(row, prompts[i_prompt]), axis=1), columns=[campo_texto])\n",
        "\n",
        "Y_true = X_test.label\n",
        "X_test = pd.DataFrame(X_test.apply(lambda row: generate_test_prompt(row, prompts[i_prompt]), axis=1), columns=[campo_texto])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a3afdd",
      "metadata": {
        "id": "a1a3afdd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76e36ee",
      "metadata": {
        "id": "f76e36ee"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b1094f",
      "metadata": {
        "id": "01b1094f"
      },
      "source": [
        "#### Conversión a Dataset de HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc67f1c",
      "metadata": {
        "id": "2fc67f1c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Convertir a Dataset de HuggingFace\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)\n",
        "test_data = Dataset.from_pandas(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26245a55",
      "metadata": {
        "id": "26245a55",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ElhWxTdoRWKA",
      "metadata": {
        "id": "ElhWxTdoRWKA"
      },
      "outputs": [],
      "source": [
        "eval_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MTgdD4rdUAmq",
      "metadata": {
        "id": "MTgdD4rdUAmq"
      },
      "outputs": [],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e0fc6b",
      "metadata": {
        "id": "91e0fc6b"
      },
      "source": [
        "## Funcion de evaluación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d98c01",
      "metadata": {
        "id": "82d98c01"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred, metodo):\n",
        "    labels = [clase_0, clase_1]\n",
        "    mapping = {0: 0, 1: 1}\n",
        "\n",
        "    # Convertir etiquetas a valores numéricos usando mapeo eficiente\n",
        "    y_true = pd.Series(y_true).map(mapping).fillna(0).astype(int)\n",
        "    y_pred = pd.Series(y_pred).map(mapping).fillna(0).astype(int)\n",
        "\n",
        "    # Calcular precisión global\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generar y mostrar el reporte de clasificación completo\n",
        "    print('\\nClassification Report:')\n",
        "    class_report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
        "    print(classification_report(y_true, y_pred, target_names=labels))\n",
        "\n",
        "    # Generar y mostrar la matriz de confusión\n",
        "    print('\\nConfusion Matrix:')\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Reorganizar la matriz de confusión si es necesario\n",
        "    conf_reordered = conf_matrix  # Puedes ajustar el orden si lo necesitas\n",
        "\n",
        "    # Graficar la matriz de confusión\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_reordered, annot=True, cmap='Reds', fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Matriz Confusion - {i_model} (Prompt {i_prompt}). {metodo}')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "\n",
        "    # Guardar la matriz de confusión\n",
        "    matriz_path = f'/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/matrices/matriz_{i_model}_prompt{i_prompt}_{metodo}.jpeg'\n",
        "    plt.savefig(matriz_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Descomponer la matriz de confusión (TN, FP, FN, TP)\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "\n",
        "    # Calcular la curva ROC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Graficar la curva ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkblue', lw=2, label='Curva ROC (AUC = {:.2f})'.format(roc_auc))\n",
        "    plt.plot([0, 1], [0, 1], color='lightgrey', linestyle='--')  # Línea diagonal\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('Tasa de Falsos Positivos')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "    plt.title(f'Curva ROC - {i_model} (Prompt {i_prompt}). {metodo}')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "\n",
        "    # Guardar la gráfica de la curva ROC\n",
        "    curva_roc_path = f'/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/curvas/curvaroc_{i_model}_prompt{i_prompt}_{metodo}.jpeg'\n",
        "    plt.savefig(curva_roc_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Extraer métricas del reporte de clasificación\n",
        "    precision_1 = class_report[clase_1]['precision']\n",
        "    recall_1 = class_report[clase_1]['recall']\n",
        "    f1_score_1 = class_report[clase_1]['f1-score']\n",
        "    support_1 = class_report[clase_1]['support']\n",
        "\n",
        "    precision_0 = class_report[clase_0]['precision']\n",
        "    recall_0 = class_report[clase_0]['recall']\n",
        "    f1_score_0 = class_report[clase_0]['f1-score']\n",
        "    support_0 = class_report[clase_0]['support']\n",
        "\n",
        "    # Asegurarse de que las métricas y la matriz se devuelvan correctamente\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_1': precision_1,\n",
        "        'recall_1': recall_1,\n",
        "        'f1_score_1': f1_score_1,\n",
        "        'support_1': support_1,\n",
        "        'precision_0': precision_0,\n",
        "        'recall_0': recall_0,\n",
        "        'f1_score_0': f1_score_0,\n",
        "        'support_0': support_0,\n",
        "        'TN': TN,\n",
        "        'FP': FP,\n",
        "        'FN': FN,\n",
        "        'TP': TP,\n",
        "        'roc_auc': roc_auc,\n",
        "        'curva_roc_path': curva_roc_path,\n",
        "        'matriz_path': matriz_path\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a0d010",
      "metadata": {
        "id": "b3a0d010"
      },
      "source": [
        "## Uso del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d81e013",
      "metadata": {
        "id": "2d81e013"
      },
      "source": [
        "#### Definición de hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac1d55c",
      "metadata": {
        "id": "0ac1d55c"
      },
      "outputs": [],
      "source": [
        "num_epochs = 4\n",
        "num_epochs_bloque = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d1d7698",
      "metadata": {
        "id": "2d1d7698"
      },
      "source": [
        "#### Enumeración de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9173184a",
      "metadata": {
        "id": "9173184a"
      },
      "outputs": [],
      "source": [
        "i_model = 0\n",
        "model_names = ['meta-llama/Llama-3.2-1B-Instruct', 'tiiuae/falcon-7b', 'BSC-LT/salamandra-7b-instruct', 'meta-llama/Llama-3.2-3B-Instruct', 'Qwen/Qwen2.5-3B-Instruct', 'google/gemma-2-2b-it', 'apry/best_2b']\n",
        "model_shorts = ['llama-1b', 'falcon-7b', 'salamandra-7b', 'llama-3b', 'qwen-3b', 'gemma-2b', 'best-2b']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fNyaQKwwoH0z",
      "metadata": {
        "id": "fNyaQKwwoH0z"
      },
      "outputs": [],
      "source": [
        "metodo1 = 'Base'\n",
        "metodo2 = 'PreprocesadoES'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e98875",
      "metadata": {
        "id": "21e98875"
      },
      "source": [
        "#### Configuración del modelo y carga de tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e656e0ee",
      "metadata": {
        "id": "e656e0ee",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Token de Hugging Face\n",
        "hf_token = \"\"\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8092eaa",
      "metadata": {
        "id": "a8092eaa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(model_name, hf_token=None, quantization=True):\n",
        "    try:\n",
        "        # Configuración de cuantización en 4 bits\n",
        "        if quantization:\n",
        "            compute_dtype = getattr(torch, \"float16\")\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=False,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=compute_dtype,\n",
        "            )\n",
        "        else:\n",
        "            bnb_config = None  # Sin cuantización\n",
        "\n",
        "        # Cargar modelo con configuración de cuantización\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            use_auth_token=hf_token,\n",
        "        )\n",
        "\n",
        "        # Configuraciones adicionales específicas del modelo\n",
        "        if hasattr(model.config, \"use_cache\"):\n",
        "            model.config.use_cache = False\n",
        "        if hasattr(model.config, \"pretraining_tp\"):\n",
        "            model.config.pretraining_tp = 1  # Modelos LLaMA\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True,\n",
        "            padding_side=\"left\",\n",
        "            add_eos_token=True,\n",
        "            use_auth_token=hf_token,\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo {model_name}: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5540e54",
      "metadata": {
        "id": "c5540e54"
      },
      "outputs": [],
      "source": [
        "# Llamada a la función\n",
        "model_name = model_names[i_model]\n",
        "model, tokenizer = load_model_and_tokenizer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de40b9a5",
      "metadata": {
        "id": "de40b9a5"
      },
      "source": [
        "#### Función de predicción"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNfssZ6UWV3D",
      "metadata": {
        "id": "MNfssZ6UWV3D"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8947335f",
      "metadata": {
        "id": "8947335f"
      },
      "outputs": [],
      "source": [
        "# Función de predicción\n",
        "def predict(X_test, model, tokenizer):\n",
        "    y_pred = []\n",
        "\n",
        "    # Crear pipeline de generación de texto\n",
        "    pipe = pipeline(\n",
        "        task=\"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=2,\n",
        "        do_sample=False,\n",
        "        return_full_text=False,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Iterar sobre los ejemplos del dataset de prueba\n",
        "    for i in tqdm(range(len(X_test))):\n",
        "        prompt = X_test[i][campo_texto]\n",
        "\n",
        "        # Agregar token especial para evitar problemas de caracteres extraños\n",
        "        result = pipe(prompt, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = result[0]['generated_text'].strip().lower()\n",
        "        print(generated_text)\n",
        "\n",
        "        # Filtrar la respuesta generada y buscar etiquetas específicas \"1\" o \"0\"\n",
        "        if clase_1.lower() in generated_text and not clase_0.lower() in generated_text:\n",
        "            y_pred.append(1)\n",
        "        elif clase_0.lower() in generated_text:\n",
        "            y_pred.append(0)\n",
        "        else:\n",
        "            # Valor predeterminado si no se encuentra ni \"1\" ni \"0\"\n",
        "            y_pred.append(0)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "# Prueba de predicción\n",
        "Y_pred = predict(test_data.select(range(10)), model, tokenizer)\n",
        "print(\"\\n\", Y_pred)\n",
        "print((Y_pred == Y_true.iloc[0:10]).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff66a7ee",
      "metadata": {
        "collapsed": true,
        "id": "ff66a7ee",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Función de evaluación\n",
        "Y_pred = predict(test_data, model, tokenizer)\n",
        "evaluate(Y_true, Y_pred, metodo1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMauEk3RVe0h",
      "metadata": {
        "id": "cMauEk3RVe0h"
      },
      "outputs": [],
      "source": [
        "# # Contar cuántos True y False hay en la comparación\n",
        "# true_count = sum(Y_pred == Y_true)\n",
        "# false_count = len(Y_pred) - true_count\n",
        "# print(f\"True count: {true_count}\")\n",
        "# print(f\"False count: {false_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c99d0b7c",
      "metadata": {
        "id": "c99d0b7c"
      },
      "source": [
        "#### Configuración de PEFT (LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6148b80",
      "metadata": {
        "id": "c6148b80"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# Configuración de LoRA (PEFT)\n",
        "def setup_peft():\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha = 16,\n",
        "        lora_dropout = 0.05,\n",
        "        r = 64,\n",
        "        bias = \"none\",\n",
        "        task_type = \"CAUSAL_LM\"\n",
        "    )\n",
        "    return peft_config\n",
        "\n",
        "peft_config = setup_peft()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fa0bbe",
      "metadata": {
        "id": "f2fa0bbe"
      },
      "source": [
        "#### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfSnAseLseRF",
      "metadata": {
        "id": "dfSnAseLseRF"
      },
      "outputs": [],
      "source": [
        "print(f\"Memoria libre: {torch.cuda.memory_reserved() / 1e9} GB\")\n",
        "print(f\"Memoria total: {torch.cuda.memory_allocated() / 1e9} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-gh_22VmsjkL",
      "metadata": {
        "id": "-gh_22VmsjkL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922242c1",
      "metadata": {
        "id": "922242c1"
      },
      "outputs": [],
      "source": [
        "# # Configuración del entrenamiento\n",
        "# def setup_training_arguments():\n",
        "#     return TrainingArguments(\n",
        "#         output_dir = \"logs\",\n",
        "#         num_train_epochs = num_epochs,\n",
        "#         per_device_train_batch_size = 8,\n",
        "#         gradient_accumulation_steps = 8,\n",
        "#         optim = \"paged_adamw_32bit\",\n",
        "#         save_steps = 0,\n",
        "#         logging_steps = 25,\n",
        "#         learning_rate = 5e-5,\n",
        "#         weight_decay = 0.01,\n",
        "#         fp16 = True,\n",
        "#         bf16 = False,\n",
        "#         max_grad_norm = 0.5,\n",
        "#         max_steps = -1,\n",
        "#         warmup_ratio = 0.1,\n",
        "#         group_by_length = True,\n",
        "#         lr_scheduler_type = \"cosine\",\n",
        "#         report_to = \"tensorboard\",\n",
        "#         save_strategy=\"epoch\",\n",
        "#         evaluation_strategy = \"epoch\",\n",
        "#         load_best_model_at_end = True,\n",
        "#         metric_for_best_model = \"eval_loss\",\n",
        "#         greater_is_better = False,\n",
        "#     )\n",
        "\n",
        "# training_arguments  =  setup_training_arguments()\n",
        "\n",
        "# # Inicialización del trainer\n",
        "# trainer = SFTTrainer(\n",
        "#     model = model,\n",
        "#     train_dataset = train_data,\n",
        "#     eval_dataset = eval_data,\n",
        "#     peft_config = peft_config,\n",
        "#     dataset_text_field = campo_texto,\n",
        "#     tokenizer = tokenizer,\n",
        "#     args = training_arguments,\n",
        "#     packing = False,\n",
        "#     max_seq_length = 256,\n",
        "#     callbacks = [\n",
        "#         EarlyStoppingCallback(early_stopping_patience = 5)\n",
        "#     ],\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CvRsKuS85zxd",
      "metadata": {
        "id": "CvRsKuS85zxd"
      },
      "outputs": [],
      "source": [
        "# trainer.train()\n",
        "# # Guardar el modelo entrenado\n",
        "# trainer.model.save_pretrained(f\"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/models/{model_shorts[i_model]}_prompt{i_prompt}_{num_epochs}_{metodo2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vIjsGJQhf3__",
      "metadata": {
        "id": "vIjsGJQhf3__"
      },
      "outputs": [],
      "source": [
        "# Configuración del entrenamiento\n",
        "def setup_training_arguments():\n",
        "    return TrainingArguments(\n",
        "        output_dir = \"logs\",\n",
        "        num_train_epochs = num_epochs_bloque,\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        optim = \"paged_adamw_32bit\",\n",
        "        save_steps = 0,\n",
        "        logging_steps = 25,\n",
        "        learning_rate = 5e-5,\n",
        "        weight_decay = 0.01,\n",
        "        fp16 = True,\n",
        "        bf16 = False,\n",
        "        max_grad_norm = 0.5,\n",
        "        max_steps = -1,\n",
        "        warmup_ratio = 0.1,\n",
        "        group_by_length = True,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        report_to = \"tensorboard\",\n",
        "        save_strategy = \"epoch\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"eval_loss\",\n",
        "        greater_is_better = False,\n",
        "    )\n",
        "\n",
        "training_arguments = setup_training_arguments()\n",
        "\n",
        "# Inicialización del trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = eval_data,\n",
        "    peft_config = peft_config,\n",
        "    dataset_text_field = campo_texto,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_arguments,\n",
        "    packing = False,\n",
        "    max_seq_length = 256,\n",
        "    callbacks = [\n",
        "        EarlyStoppingCallback(early_stopping_patience = 5)\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z5-rAb9JfeaF",
      "metadata": {
        "id": "z5-rAb9JfeaF"
      },
      "outputs": [],
      "source": [
        "# Entrenar en bloques de num_epochs_bloque épocas\n",
        "for block in range(1, (num_epochs // num_epochs_bloque) + 1):\n",
        "    print(f\"Entrenando bloque {block} de {num_epochs_bloque} épocas...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Guardar cada 2 bloques\n",
        "    if block % 2 == 0:\n",
        "        checkpoint_dir = (\n",
        "            f\"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/models_checks/\"\n",
        "            f\"{model_shorts[i_model]}_prompt{i_prompt}_{num_epochs}_epoch{block * 10}_{metodo2}\"\n",
        "        )\n",
        "        trainer.model.save_pretrained(checkpoint_dir)\n",
        "        tokenizer.save_pretrained(checkpoint_dir)\n",
        "        print(f\"Modelo temporal guardado en: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BIHKa6DugCrB",
      "metadata": {
        "id": "BIHKa6DugCrB"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo final\n",
        "final_model_dir = f\"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/models/{model_shorts[i_model]}_prompt{i_prompt}_{num_epochs}_{metodo2}\"\n",
        "trainer.model.save_pretrained(final_model_dir)\n",
        "tokenizer.save_pretrained(final_model_dir)\n",
        "\n",
        "print(f\"Modelo final guardado en: {final_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABepZhFf53Bg",
      "metadata": {
        "id": "ABepZhFf53Bg"
      },
      "outputs": [],
      "source": [
        "# # Cargar modelo entrenado\n",
        "# model_path = f\"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/models/{model_shorts[i_model]}_prompt{i_prompt}_{metodo1}\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rLQzTDM1tHV2",
      "metadata": {
        "id": "rLQzTDM1tHV2"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed65262d",
      "metadata": {
        "id": "ed65262d"
      },
      "source": [
        "#### Predicción final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f52b4dc",
      "metadata": {
        "id": "2f52b4dc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Predicción después del entrenamiento\n",
        "Y_pred = predict(test_data, model, tokenizer)\n",
        "\n",
        "# Guardar resultados en archivo CSV\n",
        "predictions = pd.DataFrame({'label': test_df[campo_texto],\n",
        "                           'Y_true': test_df[campo_etiqueta],\n",
        "                           'Y_pred': Y_pred})\n",
        "\n",
        "predictions.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/preds/test_predictions_{model_shorts[i_model]}_prompt{i_prompt}_{num_epochs}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20298501",
      "metadata": {
        "id": "20298501"
      },
      "source": [
        "#### Evaluación de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a1d55c9",
      "metadata": {
        "id": "4a1d55c9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Evaluar predicciones y obtener las métricas\n",
        "metrics = evaluate(Y_true, Y_pred, metodo2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a24a840",
      "metadata": {
        "id": "2a24a840"
      },
      "source": [
        "#### Grabado de las evaluaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22086732",
      "metadata": {
        "id": "22086732"
      },
      "outputs": [],
      "source": [
        "results_file = \"/content/drive/MyDrive/Colab Notebooks/Talleres/LLMs/resultados.csv\"\n",
        "\n",
        "# Comprobar si el archivo existe\n",
        "if os.path.exists(results_file):\n",
        "    # Si existe, cargar el CSV existente\n",
        "    df_results = pd.read_csv(results_file)\n",
        "else:\n",
        "    # Si no existe, crear un DataFrame vacío\n",
        "    df_results = pd.DataFrame(columns=['i_prompt', 'model', 'epochs', 'accuracy', 'precision_si', 'recall_si',\n",
        "                                       'f1_score_si', 'support_si', 'precision_no', 'recall_no',\n",
        "                                       'f1_score_no', 'support_no', 'TN', 'FP', 'FN', 'TP', 'roc_auc'])\n",
        "\n",
        "# Crear una fila con las métricas y la información adicional\n",
        "fila = {\n",
        "    'i_prompt': i_prompt,\n",
        "    'model': model_name,\n",
        "    'epochs': num_epochs,\n",
        "    'accuracy': metrics['accuracy'],\n",
        "    'precision_si': metrics['precision_1'],\n",
        "    'recall_si': metrics['recall_1'],\n",
        "    'f1_score_si': metrics['f1_score_1'],\n",
        "    'support_si': metrics['support_1'],\n",
        "    'precision_no': metrics['precision_0'],\n",
        "    'recall_no': metrics['recall_0'],\n",
        "    'f1_score_no': metrics['f1_score_0'],\n",
        "    'support_no': metrics['support_0'],\n",
        "    'TN': metrics['TN'],\n",
        "    'FP': metrics['FP'],\n",
        "    'FN': metrics['FN'],\n",
        "    'TP': metrics['TP'],\n",
        "    'roc_auc': metrics['roc_auc']\n",
        "}\n",
        "\n",
        "# Agregar la fila al DataFrame\n",
        "df_results = pd.concat([df_results, pd.DataFrame([fila])], ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame actualizado de nuevo en el archivo CSV\n",
        "df_results.to_csv(results_file, index=False)\n",
        "\n",
        "print(f\"Métricas guardadas correctamente en {results_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
